# -*- coding: utf-8 -*-
"""ML5-Q1-99243032-Haghighatkhah.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iURQCTRk-qC_lAk11aCBASVcAo5JGb8U
"""

import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from google.colab import drive

drive.mount('drive')

"""# Load data from a CSV file"""

dataset_address = 'drive/MyDrive/1.csv'
dataset_address2 = 'drive/MyDrive/2.csv'
dataset_address3 = 'drive/MyDrive/3.csv'
df = pd.read_csv(dataset_address)
df2 = pd.read_csv(dataset_address2)
df3 = pd.read_csv(dataset_address3)
print(df)
print(df2)
print(df3)

#df['color'] = df['color'].map({0:'r',1:'b',2:'g',3:'y',4:'black'})
df

"""#k-means"""

import numpy as np
import matplotlib.pyplot as plt

# Function to calculate Euclidean distance between two points
def euclidean_distance(x1, x2):
    return np.sqrt(np.sum((x1 - x2)**2))

# Function to initialize centroids randomly
def initialize_centroids(X, k):
    n_samples, n_features = X.shape
    centroids = np.zeros((k, n_features))
    for i in range(k):
        centroid = X[np.random.choice(range(n_samples))]
        centroids[i] = centroid
    return centroids

# Function to assign data points to the nearest centroid
def assign_clusters(X, centroids):
    n_samples = X.shape[0]
    clusters = np.zeros(n_samples)
    for i in range(n_samples):
        distances = np.zeros(k)
        for j in range(k):
            distances[j] = euclidean_distance(X[i], centroids[j])
        cluster = np.argmin(distances)
        clusters[i] = cluster
    return clusters

# Function to update centroids based on the mean of assigned data points
def update_centroids(X, clusters, k):
    n_features = X.shape[1]
    centroids = np.zeros((k, n_features))
    for i in range(k):
        cluster_points = X[clusters == i]
        centroids[i] = np.mean(cluster_points, axis=0)
    return centroids

# Function to perform k-means clustering
def kmeans(X, k, max_iterations):
    centroids = initialize_centroids(X, k)
    for _ in range(max_iterations):
        clusters = assign_clusters(X, centroids)
        prev_centroids = centroids
        centroids = update_centroids(X, clusters, k)
        if np.all(prev_centroids == centroids):
            break
    return centroids, clusters

def k_means_clustering(data, num_clusters):
    # Convert data to numpy array
    data = np.array(data)
    
    # Initialize centroids randomly
    centroids = data[random.sample(range(len(data)), num_clusters)]
    
    # Initialize cluster labels
    labels = np.zeros(len(data))
    
    # Iteratively update centroids and cluster labels
    for i in range(10):  # number of iterations
        # Update cluster labels
        for j in range(len(data)):
            distances = np.linalg.norm(data[j] - centroids, axis=1)
            labels[j] = np.argmin(distances)
        
        # Update centroids
        for k in range(num_clusters):
            centroids[k] = np.mean(data[labels == k], axis=0)
    
    # Return the predicted cluster labels
    return labels

"""# 1.csv"""

data = df
# Extract x, y, and color columns from the data
x = data['x']
y = data['y']
colors = data['color']

#Plot the points with their colors
plt.scatter(x, y, c=colors)

# Set axis labels and title
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Scatter Plot with Colors')

# Show the plot
plt.show()

from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

# Standardize the features
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[['x', 'y']])

# Define DBSCAN parameters
eps = 0.3
min_samples = 3

# Perform DBSCAN clustering
dbscan = DBSCAN(eps=eps, min_samples=min_samples)
clusters = dbscan.fit_predict(df_scaled)

# Plot the points with different colors representing different clusters
plt.scatter(df['x'], df['y'], c=df['color'])
plt.xlabel('x')
plt.ylabel('y')
plt.title('DBSCAN Clustering')
plt.show()

for i in range(1,10):      
      labels=k_means_clustering(df,i)
     # labels
      # Load data from a CSV file
      data2 = df
      data2['color']=labels
      # Extract x, y, and color columns from the data
      x = data2['x']
      y = data2['y']
      colors = data2['color']

      #Plot the points with their colors
      plt.scatter(x, y, c=colors)
      # Set axis labels and title
      plt.xlabel('X')
      plt.ylabel('Y')
      plt.title('Scatter Plot with Colors')

      # Show the plot
      plt.show()

# Create a sample dataset
data = np.array(df)

# Define the number of clusters (k) and maximum iterations
k = 5
max_iterations = 100

# Perform k-means clustering
centroids, clusters = kmeans(data, k, max_iterations)

# Plot the points and cluster centers
plt.scatter(data[:, 0], data[:, 1], c=clusters)
plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', color='red')
plt.xlabel('x')
plt.ylabel('y')
plt.title('K-means Clustering')
plt.show()

# Create a sample dataset
data = np.array(df)

# Define the number of clusters (k) and maximum iterations
#k = 8
k=5
max_iterations = 100

# Perform k-means clustering
centroids, clusters = kmeans(data, k, max_iterations)

# Plot the points and cluster centers
plt.scatter(data[:, 0], data[:, 1], c=clusters)
plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', color='red')
plt.xlabel('x')
plt.ylabel('y')
plt.title('K-means Clustering')
plt.show()

# Create a sample dataset
data = np.array(df)

# Perform k-means clustering
centroids, clusters = kmeans(data, k, max_iterations)

# Plot the points and cluster centers
plt.scatter(data[:, 0], data[:, 1], c=clusters)
plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', color='red')
plt.xlabel('x')
plt.ylabel('y')
plt.title('K-means Clustering')
plt.show()

"""#2.csv"""

data2 = df2
# Extract x, y, and color columns from the data
x = data2['x']
y = data2['y']
colors = data2['color']

#Plot the points with their colors
plt.scatter(x, y, c=colors)

# Set axis labels and title
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Scatter Plot with Colors')

# Show the plot
plt.show()

# Standardize the features
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df2[['x', 'y']])

# Define DBSCAN parameters
eps = 0.3
min_samples = 5

# Perform DBSCAN clustering
dbscan = DBSCAN(eps=eps, min_samples=min_samples)
clusters = dbscan.fit_predict(df_scaled)

# Plot the points with different colors representing different clusters
plt.scatter(df2['x'], df2['y'], c=df2['color'])
plt.xlabel('x')
plt.ylabel('y')
plt.title('DBSCAN Clustering')
plt.show()

for i in range(5,15):      
      labels=k_means_clustering(df2,i)
     # labels
      # Load data from a CSV file
      data2 = df2
      data2['color']=labels
      # Extract x, y, and color columns from the data
      x = data2['x']
      y = data2['y']
      colors = data2['color']

      #Plot the points with their colors
      plt.scatter(x, y, c=colors)
      # Set axis labels and title
      plt.xlabel('X')
      plt.ylabel('Y')
      plt.title('Scatter Plot with Colors')

      # Show the plot
      plt.show()

# Create a sample dataset
data = np.array(df2)

# Define the number of clusters (k) and maximum iterations
k = 12
max_iterations = 100

# Perform k-means clustering
centroids, clusters = kmeans(data, k, max_iterations)

# Plot the points and cluster centers
plt.scatter(data[:, 0], data[:, 1], c=clusters)
plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', color='red')
plt.xlabel('x')
plt.ylabel('y')
plt.title('K-means Clustering')
plt.show()


# Perform k-means clustering
centroids, clusters = kmeans(data, k, max_iterations)

# Plot the points and cluster centers
plt.scatter(data[:, 0], data[:, 1], c=clusters)
plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', color='red')
plt.xlabel('x')
plt.ylabel('y')
plt.title('K-means Clustering')
plt.show()

"""# 3.csv"""

data3 = df3
# Extract x, y, and color columns from the data
x = data3['x']
y = data3['y']
colors = data3['color']

#Plot the points with their colors
plt.scatter(x, y, c=colors)

# Set axis labels and title
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Scatter Plot with Colors')

# Show the plot
plt.show()

# Standardize the features
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df3[['x', 'y']])

# Define DBSCAN parameters
eps = 0.5
min_samples = 5

# Perform DBSCAN clustering
dbscan = DBSCAN(eps=eps, min_samples=min_samples)
clusters = dbscan.fit_predict(df_scaled)

# Plot the points with different colors representing different clusters
plt.scatter(df3['x'], df3['y'], c=df3['color'])
plt.xlabel('x')
plt.ylabel('y')
plt.title('DBSCAN Clustering')
plt.show()

for i in range(1,10):      
      labels=k_means_clustering(df3,i)
     # labels
      # Load data from a CSV file
      data2 = df3
      data2['color']=labels
      # Extract x, y, and color columns from the data
      x = data2['x']
      y = data2['y']
      colors = data2['color']

      #Plot the points with their colors
      plt.scatter(x, y, c=colors)
      # Set axis labels and title
      plt.xlabel('X')
      plt.ylabel('Y')
      plt.title('Scatter Plot with Colors')

      # Show the plot
      plt.show()

# Create a sample dataset
data = np.array(df3)

# Define the number of clusters (k) and maximum iterations
k =4
max_iterations = 100

# Perform k-means clustering
centroids, clusters = kmeans(data, k, max_iterations)

# Plot the points and cluster centers
plt.scatter(data[:, 0], data[:, 1], c=clusters)
plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', color='red')
plt.xlabel('x')
plt.ylabel('y')
plt.title('K-means Clustering')
plt.show()


# Perform k-means clustering
centroids, clusters = kmeans(data, k, max_iterations)

# Plot the points and cluster centers
plt.scatter(data[:, 0], data[:, 1], c=clusters)
plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', color='red')
plt.xlabel('x')
plt.ylabel('y')
plt.title('K-means Clustering')
plt.show()

###################
# -*- coding: utf-8 -*-
"""ML5-Q2-99243032-Haghighatkhah.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B_k8yG11P_KA3RmjKi5vE1Dmof8r-iE9
"""

import numpy as np
import pandas as pd #
import os
import numpy as np
import scipy.io
import cv2
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split

!wget http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz
!tar -xzf 102flowers.tgz
!rm 102flowers.tgz
!wget http://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat

#set labels
img_labels = scipy.io.loadmat("imagelabels.mat")
img_labels = img_labels["labels"]
img_labels = img_labels[0]
for i in range(len(img_labels)):
  img_labels[i] = img_labels[i] - 1

import numpy as np
import cv2
import os

train_x = []
train_y = []
dir = "jpg/"
#iterate over the files in the specified directory
for imgs in os.listdir(dir):
    img_num = int(imgs[7:11]) - 1
    #appends the image label corresponding to the current image to the train_y list
    train_y.append(img_labels[img_num])
    image = cv2.imread(os.path.join(dir, imgs))
    #This step ensures that all images have the same dimensions.
    resized = cv2.resize(image, (150, 150))
    #ormalization process scales the pixel values of the image between 0 and 1
    normalized_img = cv2.normalize(resized, None, alpha=0, beta=1,
                                  norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)
    #adds the normalized image to the train_x list
    train_x.append(normalized_img)
train_x = np.array(train_x)

X_train, x_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2, random_state=10)

print('X_train Shape: ­{}'.format(X_train.shape))

#normalizing the pixel values of the training and test images by dividing each pixel value by 255.0.
#ensure that all pixel values are within a range of 0 to 1, which makes it easier for the model to learn.
training_images= X_train/255.0
test_images=x_test/255.0
'''
reshaping the training and test images to have a shape of (number of images, height, width, channels),
where the height and width are 150 pixels each and the number of channels is 3 (for RGB images).
'''
training_images = X_train.reshape((6551,150,150,3))
valx = x_test.reshape((1638,150,150,3))
#Converting to categorical
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

"""#ANN implementation
Define the model architecture: The next step is to define the architecture of the model. The Sequential model is instantiated, which represents a linear stack of layers.

Flatten layer: The Flatten layer is added as the first layer in the model. This layer flattens the input shape of (150, 150, 3) into a one-dimensional vector of size 67500. It is typically used to transform multi-dimensional input into a single dimension for compatibility with subsequent layers.

Dense layers: Three Dense layers are added to the model. These are fully connected layers, where each neuron is connected to every neuron in the previous layer. The first Dense layer has 128 neurons and uses the ReLU activation function, which introduces non-linearity to the model. The second Dense layer also has 128 neurons and uses ReLU activation.

Output layer: The final Dense layer is added with 102 neurons, corresponding to the number of classes in the classification problem. The activation function used is softmax, which converts the output into probabilities, allowing the model to provide a probability distribution over the classes.
"""

from keras.models import Sequential
from keras.layers import Dense, Flatten

# Define the model architecture
model = Sequential()
model.add(Flatten(input_shape=(150, 150, 3)))
model.add(Dense(128, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(102, activation='softmax'))

# Compile the model
model.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(training_images, y_train, epochs=50, batch_size=32, validation_data=(valx, y_test))

model.summary()

# Plot the training history
import matplotlib.pyplot as plt

# Plot the training and validation accuracy values
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

# Plot the training and validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

"""#CNN implementation
In this implementation, we define an ANN model with three convolutional layers, followed by two fully connected layers. The convolutional layers extract features from the input images, while the fully connected layers perform the classification. The architecture of the model is as follows:

- Convolutional layer with 32 filters, kernel size of 3x3, and ReLU activation
- Max pooling layer with pool size of 2x2
- Convolutional layer with 64 filters, kernel size of 3x3, and ReLU activation
- Max pooling layer with pool size of 2x2
- Convolutional layer with 128 filters, kernel size of 3x3, and ReLU activation
- Max pooling layer with pool size of 2x2
- Flatten layer
- Fully connected layer with 512 units and ReLU activation
- Dropout layer with a rate of 0.5
- Fully connected layer with 102 units (one for each class) and softmax activation

The model is compiled with the categorical cross-entropy loss function, the AdaGrade optimizer with a learning rate of 0.001, and the accuracy metric.
"""

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.optimizers import Adagrad
import matplotlib.pyplot as plt

# Define the architecture of the model
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(102, activation='softmax'))

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer=Adagrad(lr=0.001), metrics=['accuracy'])

# Train the model
history = model.fit(training_images, y_train, batch_size=32, epochs=50, validation_data=(valx, y_test))

model.summary()

# Plot the training and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot the training and validation accuracy
plt.plot(history.history['acc'], label='Training Accuracy')
plt.plot(history.history['val_acc'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""by using Adam optimizer, better acc will be achieve

test
"""

model_cnn = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(256, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(102, activation='softmax')
])
# Compile the CNN model
model_cnn.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the CNN model
history_cnn = model_cnn.fit(training_images, y_train, batch_size=32, epochs=50, validation_data=(valx, y_test))

import matplotlib.pyplot as plt

# Plot the training and validation accuracy for the CNN model
plt.plot(history_cnn.history['acc'])
plt.plot(history_cnn.history['val_acc'])
plt.title('CNN Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Plot the training and validation loss for the CNN model
plt.plot(history_cnn.history['loss'])
plt.plot(history_cnn.history['val_loss'])
plt.title('CNN Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

from keras.models import Sequential
from keras.layers import Dense

# Define the ANN model
ann_model = Sequential()
ann_model.add(Flatten(input_shape=(150, 150, 3)))
ann_model.add(Dense(512, activation='relu'))
ann_model.add(Dense(256, activation='relu'))
ann_model.add(Dense(102, activation='softmax'))

# Compile the model
ann_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Print the summary of the model
ann_model.summary()

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# Define the CNN model
cnn_model = Sequential()
cnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))
cnn_model.add(MaxPooling2D((2, 2)))
cnn_model.add(Conv2D(64, (3, 3), activation='relu'))
cnn_model.add(MaxPooling2D((2, 2)))
cnn_model.add(Conv2D(128, (3, 3), activation='relu'))
cnn_model.add(MaxPooling2D((2, 2)))
cnn_model.add(Conv2D(128, (3, 3), activation='relu'))
cnn_model.add(MaxPooling2D((2, 2)))
cnn_model.add(Flatten())
cnn_model.add(Dense(512, activation='relu'))
cnn_model.add(Dense(102, activation='softmax'))

# Compile the model
cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Print the summary of the model
cnn_model.summary()

import matplotlib.pyplot as plt

# Train the CNN model
cnn_history = cnn_model.fit(training_images, y_train, validation_data=(valx, y_test), epochs=30, batch_size=32)

# Train the ANN model
ann_history = ann_model.fit(training_images, y_train,
                            validation_data=(valx, y_test),
                            epochs=30, batch_size=32)

# Plot the training error, testing error, and accuracy for the CNN model
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(cnn_history.history['loss'], label='Training Error')
plt.plot(cnn_history.history['val_loss'], label='Testing Error')
plt.xlabel('Epochs')
plt.ylabel('Error')
plt.title('CNN Model: Training Error vs Testing Error')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(cnn_history.history['acc'], label='Training Accuracy')
plt.plot(cnn_history.history['val_acc'], label='Testing Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('CNN Model: Training Accuracy vs Testing Accuracy')
plt.legend()
plt.show()

# Plot the training error, testing error, and accuracy for the ANN model
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(ann_history.history['loss'], label='Training Error')
plt.plot(ann_history.history['val_loss'], label='Testing Error')
plt.xlabel('Epochs')
plt.ylabel('Error')
plt.title('ANN Model: Training Error vs Testing Error')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(ann_history.history['acc'], label='Training Accuracy')
plt.plot(ann_history.history['val_acc'], label='Testing Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('ANN Model: Training Accuracy vs Testing Accuracy')
plt.legend()
plt.show()