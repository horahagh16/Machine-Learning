# -*- coding: utf-8 -*-
"""ML3-99243032-haghighatkhah.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FHF5glFPbAVAa39XRs1gnkftAt8QPvN7


!pip install sweetviz
"""
# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from sklearn import linear_model,svm
from sklearn.metrics import average_precision_score
from sklearn.metrics import precision_recall_curve

from google.colab import drive

drive.mount('drive')

dataset_address = 'drive/MyDrive/loan.csv'
df = pd.read_csv(dataset_address)
df

"""check missing value """

df.isnull().sum()

"""outlier"""

import seaborn as sns
sns.boxplot(df['Funded Amount Investor'])

"""outlire</br>I checked values by Excel but I dont have much information about these values so i think there were no unnormal value

data encoding
"""

df['Grade'] = df['Grade'].map({'A':7,'B':6,'C':5,'D':4,'E':3,'F':2,'G':1})
df["Employment Duration"] = df["Employment Duration"].map({"MORTGAGE":3,"RENT":2,"OWN":1})

vs = pd.get_dummies(df['Verification Status'])
df = pd.concat([df,vs],axis=1)

#at = pd.get_dummies(df['Application Type'])
#df = pd.concat([df,at],axis=1)
df['Application Type'] =df['Application Type'].astype('category').cat.codes

#lt = pd.get_dummies(df['Loan Title'])
#df = pd.concat([df,lt],axis=1)

#lls = pd.get_dummies(df['Initial List Status'])
#df = pd.concat([df,lls],axis=1)
df['Initial List Status'] =df['Initial List Status'].astype('category').cat.codes

#delete some features
#del df['Application Type']
del df['Verification Status']
del df['Loan Title']
#del df['Initial List Status']
del df['id']
print("Current shape of dataset :",df.shape)
df.head()

"""find corr then delete some unnecessary features"""

corr = df[list(df.columns)].corr()['Status'][:]
corr

df.corr()

del df['Revolving Utilities']
del df['Payment Plan']
del df['Accounts Delinquent']
del df['Batch Enrolled']
del df['Sub Grade']
print("Current shape of dataset :",df.shape)
df.head()

df.corr()

"""preprocessing and data scaler"""

features=list(df.columns)
features.remove('Status')

from sklearn.preprocessing import StandardScaler

df[features] = StandardScaler().fit_transform(df[features])
print(df['Status'].value_counts())
df

# Get number of positve and negative examples
pos = df[df["Status"] == 0].shape[0]
neg = df[df["Status"] == 1].shape[0]
print(f"Positive examples = {pos}")
print(f"Negative examples = {neg}")
print(f"Proportion of positive to negative examples = {(pos / neg) * 100:.2f}%")

"""imbalance dataset"""

status_0 = df[df["Status"]==0]
status_1 = df[df["Status"]==1]
subset_of_status_0 = status_0.sample(n=8000)
subset_of_status_1 = status_1.sample(n=6000)
new_df = pd.concat([subset_of_status_1, subset_of_status_0])
new_df = new_df.sample(frac=1).reset_index(drop=True) #shuffel
print("Current shape of dataset :",new_df.shape)
new_df.head()

from sklearn.model_selection import train_test_split

X = new_df.drop(['Status'],axis=1)
y = new_df['Status']
#80-20
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)

"""#Logistic Regression"""

#balanced data
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import average_precision_score

# Fit logistic regression model
model = LogisticRegression(max_iter=10000)
model.fit(X_train, y_train)

# Make predictions on test data
y_pred = model.predict(X_test)

# Calculate accuracy score
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)
average_precision = average_precision_score(y_test, y_pred)
print("Accuracy:", accuracy)#preferred
print("roc_auc:", roc_auc) 
print("average_precision:", average_precision)

#imbalanced data
from sklearn.model_selection import train_test_split

X = df.drop(['Status'],axis=1)
y = df['Status']
#80-20
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import average_precision_score

# Fit logistic regression model
model = LogisticRegression(max_iter=10000)
model.fit(X_train, y_train)

# Make predictions on test data
y_pred = model.predict(X_test)

# Calculate accuracy score
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)
average_precision = average_precision_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("roc_auc:", roc_auc) #preferred
print("average_precision:", average_precision)

from sklearn.svm import SVC
from sklearn.model_selection import learning_curve
from sklearn.model_selection import ShuffleSplit

def plot_learning_curve(estimator, title, X, y, cv=None,
                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):
    plt.figure()
    plt.title(title)
    plt.xlabel("Training examples")
    plt.ylabel("Score")
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1) #enheraf meyar
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    plt.grid()
    
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1,
                     color="b")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, color="b",
             label="Training score")
    plt.plot(train_sizes, test_scores_mean, color="g",
             label="Cross validation score")
    plt.legend(loc="best")
    return plt

X, y = new_df.iloc[:,:8].values, new_df.iloc[:,8].values #column 9 is for status
title = "Learning Curves (Logistic Regression)"
# Cross validation with 100 iterations 
# each time with 20% data randomly selected as a validation set.
cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)
estimator = linear_model.LogisticRegression()
plot_learning_curve(estimator, title, X, y, cv=cv, n_jobs=4)
plt.show()

"""پاسخ بخش ب: چون این دیتاست نامتعادل است و تعداد داده های 0 و 1 تفاوت فاحشی دارند و ما این موضوع را در نظر گرفتیم و داده ها را متعادل کردیم پس نتایج خوب و قبل قبول اند.اگر هر دو امتیاز اعتبار سنجی و امتیاز آموزشی با افزایش اندازه مجموعه آموزشی به مقدار بسیار کم همگرا شوند، از داده های آموزشی بیشتر بهره نخواهیم برد.ما در اینجا با توجه به تمودار و همگرایی در مقدار پایین متوجه میشویم که ب افزایش دیتا از حدود  10000 دیگر بهبودی نداریم.

#Decision Tree Classifier and  Bagging Classifier
"""

#balanced data
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

X = new_df.drop(['Status'],axis=1)
y = new_df['Status']

# Instantiate dt
dt = DecisionTreeClassifier(random_state=1)

# Instantiate bc
bc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)
# Fit bc to the training set
bc.fit(X_train, y_train)

# Predict test set labels
y_pred = bc.predict(X_test)

# Evaluate acc_test
acc_test = accuracy_score(y_test, y_pred)
print('Test set accuracy of bc: {:.2f}'.format(acc_test))
dt.fit(X_train, y_train)

y_pred_dt = dt.predict(X_test)

accuracy = accuracy_score(y_test, y_pred_dt)
roc_auc = roc_auc_score(y_test, y_pred_dt)
average_precision = average_precision_score(y_test, y_pred_dt)
print("Accuracy:", accuracy)
print("roc_auc:", roc_auc) #preferred
print("average_precision:", average_precision)

#imbalance data
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

X = df.drop(['Status'],axis=1)
y = df['Status']

# Instantiate dt
dt = DecisionTreeClassifier(random_state=1)

# Instantiate bc
bc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)
# Fit bc to the training set
bc.fit(X_train, y_train)

# Predict test set labels
y_pred = bc.predict(X_test)

# Evaluate acc_test
acc_test = accuracy_score(y_test, y_pred)
print('Test set accuracy of bc: {:.2f}'.format(acc_test))
dt.fit(X_train, y_train)

y_pred_dt = dt.predict(X_test)

accuracy = accuracy_score(y_test, y_pred_dt)
roc_auc = roc_auc_score(y_test, y_pred_dt)
average_precision = average_precision_score(y_test, y_pred_dt)
print("Accuracy:", accuracy)
print("roc_auc:", roc_auc) #preferred
print("average_precision:", average_precision)

"""#Random Forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
X = new_df.drop(['Status'],axis=1)
y = new_df['Status']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

clf = RandomForestClassifier() 
clf.fit(X_train, y_train) 
y_pred_rf = clf.predict(X_test)

confusion_matrix(y_test, y_pred_rf)

accuracy = accuracy_score(y_test, y_pred_rf)
roc_auc = roc_auc_score(y_test, y_pred_rf)
average_precision = average_precision_score(y_test, y_pred_rf)
print("Accuracy:", accuracy)#preferred
print("roc_auc:", roc_auc) 
print("average_precision:", average_precision)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
X = df.drop(['Status'],axis=1)
y = df['Status']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

clf = RandomForestClassifier() 
clf.fit(X_train, y_train) 
y_pred_rf = clf.predict(X_test)

confusion_matrix(y_test, y_pred_rf)

accuracy = accuracy_score(y_test, y_pred_rf)
roc_auc = roc_auc_score(y_test, y_pred_rf)
average_precision = average_precision_score(y_test, y_pred_rf)
print("Accuracy:", accuracy)
print("roc_auc:", roc_auc) #preferred
print("average_precision:", average_precision)

'''
import sweetviz as sv
my_report = sv.analyze(df)
my_report.show_html()'''